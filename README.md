This project explores a deep learning-based approach to image captioning aimed at generating natural language 
descriptions of images. The proposed model integrates Convolutional Neural Networks (CNNs) for visual feature extraction
and Recurrent Neural Networks (RNNs) with Long Short-Term
Memory (LSTM) units for language modeling. Key features
include multilingual translation, text-to-speech (TTS), and Braille
encoding, enhancing accessibility for visually impaired users.
The model was trained on the Flickr8k dataset and evaluated
using BLEU metric and human feedback, demonstrating strong
performance on simple images but challenges with complex
scenes
